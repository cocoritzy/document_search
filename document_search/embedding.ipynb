{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0231b208",
   "metadata": {},
   "source": [
    "# Download embeddings model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2c76e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/week_2/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa06c8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/week_2/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "from model import CBOW\n",
    "\n",
    "# model withou title hackers\n",
    "model_path = hf_hub_download(repo_id=\"cocoritzy/cbow-upvotes_model\", filename=\"cbow_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48427377",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load(model_path, map_location=device) #A checkpoint is a file that saves the state of your model (\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cc96c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('embeddings.weight', tensor([[-6.4979e-01, -5.7941e-01,  6.6106e-01,  ..., -2.2381e+00,\n",
      "         -4.7060e-04,  1.5041e-01],\n",
      "        [-4.9834e-01, -1.3562e+00,  5.9971e-02,  ...,  2.2810e-02,\n",
      "         -4.2601e+00,  1.6657e+00],\n",
      "        [-3.5826e-01,  1.8044e+00,  3.4292e-01,  ..., -4.9925e-01,\n",
      "          6.3616e-02, -2.0757e-01],\n",
      "        ...,\n",
      "        [ 3.0328e+00,  2.2346e+00, -4.0947e+00,  ...,  1.5389e+00,\n",
      "         -3.1774e+00,  1.5126e+00],\n",
      "        [ 9.5289e-01, -7.5809e-01, -1.0489e+00,  ...,  2.7495e+00,\n",
      "         -2.2415e+00,  6.3896e-01],\n",
      "        [ 6.0927e-01, -3.6017e-01, -2.4570e+00,  ...,  2.5984e+00,\n",
      "         -1.5189e+00, -1.4678e+00]], device='cuda:0')), ('linear.weight', tensor([[-0.9190, -3.4283, -1.5246,  ...,  0.7419,  2.3657, -0.5639],\n",
      "        [-0.9271, -3.3398, -1.2597,  ...,  0.3505,  2.7461, -0.6402],\n",
      "        [-1.0051, -3.6519, -1.2860,  ...,  0.3628,  2.5529, -0.4234],\n",
      "        ...,\n",
      "        [-1.0449, -2.6130, -1.2064,  ...,  0.3532,  2.4671, -0.5399],\n",
      "        [-0.7596, -3.6919, -1.1978,  ...,  0.1844,  2.4805, -0.5774],\n",
      "        [-0.6487, -3.7323, -1.4123,  ...,  0.1245,  2.9077, -0.6741]],\n",
      "       device='cuda:0')), ('linear.bias', tensor([ -1.7899,  -2.3857,  -2.0153,  ..., -16.6750, -17.6582, -16.5161],\n",
      "       device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a038b349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embeddings): Embedding(30000, 100)\n",
       "  (linear): Linear(in_features=100, out_features=30000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load dim\n",
    "token_to_index = checkpoint[\"token_to_index\"]\n",
    "\n",
    "embedding_dim= checkpoint[\"embedding_dim\"]\n",
    "vocab_size = len(token_to_index)  # fill in actual size\n",
    "\n",
    "embedding_dim = checkpoint['embedding_dim']\n",
    "\n",
    "# Load the model architecture\n",
    "model = CBOW(voc=vocab_size, emb=embedding_dim)\n",
    "\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    " # it contains the model's parameters and other information needed to resume training or make predictions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7961b8",
   "metadata": {},
   "source": [
    "# Load query and title documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40da2c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query_id', 'query', 'positive_passage', 'negative_passage',\n",
       "       'negative_index_in_group'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cocoritzy/week_2_triplet_dataset_hard_negatives\")\n",
    "\n",
    "split_data = dataset[\"train\"].train_test_split(test_size=0.2, seed=42) # 80% train, 20% test\n",
    "\n",
    "\n",
    "df_train = split_data[\"train\"].to_pandas()\n",
    "df_train.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e86b3",
   "metadata": {},
   "source": [
    "# Convert title to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a214749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c394b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_to_embedding(title):\n",
    "    tokens = title.lower().split()\n",
    "    indices = [token_to_index.get(tok, 0) for tok in tokens]  # 0 for unknowns\n",
    "\n",
    "    indices_tensor = torch.tensor(indices, dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeds = embedding_layer(indices_tensor) # [num_tokens, embedding_dim]\n",
    "        return embeds.mean(dim=0) # average pooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b36c4955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nagative: FLOT/FEBA may include covering and screening forces. The Forward Line of Enemy Troops (FLET) , is the FEBA from the enemy's perspective. The adjective variant of the term front line is used to describe materiel or personnel intended for forward use-at sea, on land or in the air-i.e. at the front line. positive: forward line of own troops (FLOT) (JP 1-02) - A line which indicates the most forward positions of friendly forces in any kind of military operation at a specific time. The FLOT normally identifies the forward location of covering and screening forces. (Army) - The FLOT may be at, beyond, or short of the FEBA. An enemy FLOT indicates the forward most position of hostile forces. (See also line of contact (LC).) See FMs 1-111, 6-20 series, 7-20, 7-30, 71-100, 71-123, 100-5, and 100-15. An area in hostile or insurgent territory which has a 360-degree defense and which supports combat patrols or larger operations with combat support and combat service support assets. (See also echelonment .) See FM 71-100-3.\n"
     ]
    }
   ],
   "source": [
    "negative = df_train.iloc[12][\"negative_passage\"]\n",
    "postive = df_train.iloc[12][\"positive_passage\"]\n",
    "\n",
    "print(\"nagative:\",negative,\"positive:\",postive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "715557b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_train.iloc[12][\"query\"]\n",
    "negative = df_train.iloc[12][\"negative_passage\"]\n",
    "postive = df_train.iloc[12][\"positive_passage\"]\n",
    "\n",
    "a = title_to_embedding(query)\n",
    "b = title_to_embedding(\"1\")\n",
    "c = title_to_embedding(postive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "590ac556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return F.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f91fb7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00491216778755188 0.3279302716255188\n"
     ]
    }
   ],
   "source": [
    "sim_pos = cosine_similarity(a, b)\n",
    "sim_neg = cosine_similarity(a, c)\n",
    "print(sim_pos, sim_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ab6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
